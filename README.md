# Obstacle Evader Mobile Robot

<p align="center">
  <img src="data/project_title.png" alt="Project Logo Cover" width="1500"/>
</p>


---------------------------------------------


## About this Project:
This project introduces the HK Bot, an advanced robotic system designed to autonomously navigate and avoid obstacles. It is integrated with YdLiDAR technology for precise environmental scanning and employs the Robot Operating System (ROS) for robust and flexible robot programming. Additionally, Python is utilized, likely for scripting and automation, enhancing the bot's capability to operate intelligently in dynamic settings.

<p align="center">
  <img src="data/project_logo.png" alt="Project Title" width="1500"/>
</p>


---------------------------------------------


## What is YdLidar:
YdLiDAR is a type of Light Detection and Ranging (LiDAR) sensor designed for use in robotics and automation. LiDAR sensors measure distances by illuminating targets with laser light and measuring the reflection with a sensor. YdLiDAR sensors are known for their cost-effectiveness and compact size, making them suitable for applications like obstacle avoidance, area mapping, and robot navigation where precise distance measurements and environmental awareness are crucial. 

These sensors can rapidly scan their surroundings to create real-time maps that robots use for path planning and obstacle avoidance, playing a critical role in the field of robotics.

<p align="center">
  <img src="data/YDLidar.png" alt="2" width="1500"/>
</p>


---------------------------------------------


## What is Proximal Policy Optimization (PPO):
Proximal Policy Optimization (PPO) is a policy gradient method for reinforcement learning which alternates between sampling data through interaction with the environment and optimizing a "surrogate" objective function using stochastic gradient ascent. Developed by OpenAI, PPO aims to improve upon the stability and sample efficiency of previous methods like Trust Region Policy Optimization (TRPO) but with simpler implementation and better general performance.


---------------------------------------------


## Key aspects of  include:

    • Clipped Objective: Limits policy updates to prevent excessive changes.
    • Multiple Updates: Allows several mini-batch updates per data sample for better efficiency.
    • KL Penalty/Clipping: Ensures policy updates stay within a "safe" range to maintain training stability.
    • Advantage: PPO is favored for its simplicity, efficiency, and consistent performance across various RL tasks.

<p align="center">
  <img src="data/3.png" alt="3" width="1500"/>
</p>


---------------------------------------------


## Results:

<p align="center">
  <img src="data/4.png" alt="4" width="1500"/>
</p>


---------------------------------------------


## My Project Video Demonstration:

<p align="center">
  <a href="https://www.linkedin.com/posts/tvharikrishna_reinforcementlearning-machinelearning-neuralnetwork-activity-7118597093476196352-Q_cM?utm_source=share&utm_medium=member_desktop">
    <img src="https://img.shields.io/badge/Video-Watch How AI Slab is Balancing-blue" alt="Video"/>
  </a>
</p>

---------------------------------------------
